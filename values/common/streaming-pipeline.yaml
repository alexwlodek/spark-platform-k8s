generator:
  enabled: true
  replicas: 1
  image:
    repository: ghcr.io/alexwlodek/order-generator
    tag: "latest"
    pullPolicy: IfNotPresent
  orderRatePerSecond: 20
  minAmount: 10
  maxAmount: 500
  kafka:
    bootstrapServers: streaming-kafka.apps.svc.cluster.local:9092
    topic: orders
    acks: "1"
    compressionType: none
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 300m
      memory: 256Mi

sparkApplication:
  enabled: true
  labels: {}
  annotations: {}
  type: Python
  mode: cluster
  sparkVersion: "3.5.2"
  pythonVersion: "3"
  mainApplicationFile: local:///opt/spark/work-dir/streaming_job.py
  image:
    repository: ghcr.io/alexwlodek/spark-demo-job
    tag: "latest"
    pullPolicy: IfNotPresent
  arguments:
    - --kafka-bootstrap-servers
    - streaming-kafka.apps.svc.cluster.local:9092
    - --kafka-topic
    - orders
    - --window-duration
    - 1 minute
    - --watermark-delay
    - 2 minutes
    - --checkpoint-location
    - s3a://streaming-lake/checkpoints/orders-v1
    - --output-path
    - s3a://streaming-lake/silver/orders_per_minute
    - --query-name
    - orders_revenue_per_minute
    - --trigger-processing-time
    - 10 seconds
    - --metrics-port
    - "8090"
  sparkConf:
    spark.ui.prometheus.enabled: "true"
    spark.sql.shuffle.partitions: "2"
    spark.sql.streaming.metricsEnabled: "true"
    spark.jars.ivy: /tmp/.ivy2
    spark.jars.packages: org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262
    spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
    spark.hadoop.fs.s3a.endpoint: http://streaming-minio.apps.svc.cluster.local:9000
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.hadoop.fs.s3a.connection.ssl.enabled: "false"
    spark.hadoop.fs.s3a.aws.credentials.provider: org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
    spark.hadoop.fs.s3a.access.key: minio
    spark.hadoop.fs.s3a.secret.key: minio123
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 30
    onSubmissionFailureRetries: 5
    onSubmissionFailureRetryInterval: 20
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "1024m"
    serviceAccount: spark-operator-spark
    labels:
      streaming-query: orders-streaming
    env:
      - name: PYTHONUNBUFFERED
        value: "1"
  executor:
    instances: 1
    cores: 1
    coreLimit: "1200m"
    memory: "1024m"
    deleteOnTermination: true
    labels:
      streaming-query: orders-streaming

metrics:
  podMonitor:
    enabled: true
    namespace: ""
    labels:
      release: kube-prometheus-stack
    interval: 15s
    scrapeTimeout: 10s
    selector:
      sparkRole: driver
      streamingQuery: orders-streaming
    endpoints:
      - path: /metrics/prometheus
        targetPort: 4040
      - path: /metrics/executors/prometheus
        targetPort: 4040
      - path: /metrics
        targetPort: 8090

grafanaDashboard:
  enabled: true
  namespace: ""
  labels: {}

prometheusRule:
  enabled: true
  namespace: ""
  labels:
    release: kube-prometheus-stack
  queryLabel: orders_revenue_per_minute
  alerts:
    jobFailed:
      enabled: true
      for: 5m
    lagGrowing:
      enabled: true
      for: 10m
      thresholdSeconds: 120
